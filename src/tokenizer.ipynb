{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Tokenizer Vokabular generieren\n",
    "Verben = set(pos_vocab)\n",
    "P(Verben) = {}\n",
    "for target in Verben:\n",
    "    for verb in Verben:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Für alle tokens mit POS = verb:\n",
    "    Sortiere nach Silbenanzahl\n",
    "\n",
    "Für jede Silbenanzahl:\n",
    "    Für jedes Verb V1 in Verben:\n",
    "        Für jedes Verb V2 in Verben:\n",
    "            Vergleiche Anfang oder Ende (V1, V2)\n",
    "            Wenn gleich:\n",
    "                Lege Map mit binärem Vergleich für jedes Zeichen an\n",
    "                Padde die Map auf die Länge des Targets\n",
    "                Sammle Map\n",
    "        Entferne Übereinstimmmungen innerhalb des Worts\n",
    "        Ignoriere unäre Übereinstimmungen  (Summe Map > 1)\n",
    "        Addiere identische Maps\n",
    "        Behalte n häufigste Maps\n",
    "        Map dekodieren, in Tokenizer-Vokabular schreiben\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Wenn POS:\n",
    "    Finde längsten Substring aus TK-Vokabular (Stämme)\n",
    "    Präfix:\n",
    "        Rekursiv längste Substrings bis komplett segmentiert\n",
    "    Suffix:\n",
    "        Rekursiv längste Substrings bis komplett segmentiert\n",
    "    Rest:\n",
    "        Aus atomarem Vokabular"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5010\n",
      "2 27261\n",
      "3 30123\n",
      "4 14810\n",
      "5 4455\n",
      "6 1005\n",
      "7 193\n",
      "8 32\n"
     ]
    }
   ],
   "source": [
    "from src import text_utilities as tu\n",
    "wordset = tu.from_path('./../data/experiment/verbs')\n",
    "verb_dict = tu.count_syllables(wordset)\n",
    "verb_dict.keys()\n",
    "for i in range(1,9): # von mächtigster menge absteigend morpheme analysieren\n",
    "    print(i, len(verb_dict[i]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "matches = []\n",
    "syllables = 3\n",
    "target = \"versagen\"\n",
    "\n",
    "for v in verb_dict[syllables]:\n",
    "    sm = SequenceMatcher(None, target, v)\n",
    "    matches.append((sm.find_longest_match(alo=0, ahi=len(target), blo=0, bhi=len(v)), target, v))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def max_subword(w1: str, w2: str) -> str:\n",
    "    seq = SequenceMatcher(None, w1, w2)\n",
    "    match = seq.find_longest_match(alo=0,  ahi=len(seq.a), blo=0, bhi=len(seq.b))\n",
    "    #print(w1[match.a:(match.a+match.size)], w2[match.b:(match.b+match.size)])\n",
    "    if match.size > 0 and match.a > 0:\n",
    "        return w1[match.a:(match.a+match.size)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "lex = []\n",
    "target = \"verzeichnen\"\n",
    "for i in verb_dict:\n",
    "    [lex.append(max_subword(target, v)) for v in verb_dict[i] if max_subword(target, v)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "[('er', 26237),\n ('e', 16981),\n ('ch', 9876),\n ('ei', 6709),\n ('en', 3868),\n ('r', 2653),\n ('ich', 879),\n ('i', 816),\n ('ze', 694),\n ('z', 659),\n ('ic', 626),\n ('eich', 605),\n ('ne', 581),\n ('chn', 538),\n ('n', 529),\n ('h', 453),\n ('chne', 323),\n ('hn', 298),\n ('c', 274),\n ('rz', 230),\n ('erz', 219),\n ('nen', 121),\n ('rze', 94),\n ('hne', 86),\n ('zeichne', 65),\n ('zei', 63),\n ('erze', 35),\n ('zeichn', 13),\n ('hnen', 11),\n ('erzeichne', 9),\n ('erzei', 5),\n ('erzeichn', 2),\n ('chnen', 2),\n ('zeichnen', 2),\n ('rzei', 1),\n ('rzeichn', 1)]"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections as cl\n",
    "ll = cl.Counter(lex).most_common()\n",
    "ll[:50]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
