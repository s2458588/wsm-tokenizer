{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch.cuda\n",
    "\n",
    "import wm_tokenizer\n",
    "import text_utilities as tu\n",
    "from HanTa import HanoverTagger as ht\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments , AutoModelForMaskedLM  # , AutoTokenizer, BertForMaskedLM\n",
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "import sklearn\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "vd = tu.VerbDict(\"../new_tokenizer/fun_vocab_raw.txt\", \"../new_tokenizer/lex_vocab_raw.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "wmt = wm_tokenizer.WordmapTokenizer(\n",
    "    bert_pretokenizer=pre_tokenizers.BertPreTokenizer(),\n",
    "    bert_tokenizer=BertTokenizer.from_pretrained(\"bert-base-german-cased\"),\n",
    "    hantatagger=ht.HanoverTagger('morphmodel_ger.pgz'),\n",
    "    vocab=vd.lmfm\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def wm_tokenize(data):\n",
    "    return wmt.wordmap2tokenizer(data['text'], pos_tag=\"V\", vocab=wmt.vocab, pt=wmt.bert_pretokenizer, tk=wmt.bert_tokenizer, tg=wmt.hantatagger)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"TODO: Fix UNK tokens bei wmt.SequenceTokenizer\"\"\"\n",
    "\n",
    "    files =  tu.files_from_path(\"../data/oscar/to_lines\", full_path=True)\n",
    "    dataset = datasets.load_dataset(\"text\", data_files=files, split=\"train\")\n",
    "    dataset = dataset.train_test_split(train_size=500000, test_size=75000, writer_batch_size=1000)\n",
    "    metric = datasets.load_metric('glue', 'mrpc', keep_in_memory=True)\n",
    "\n",
    "\n",
    "    tokenized_dataset = dataset.map(wm_tokenize, batched=True, batch_size=1000)\n",
    "\n",
    "    # recommendations: https://github.com/google-research/bert\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./out/model_out',  # output directory\n",
    "        num_train_epochs=4,  # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        logging_dir='./out/model_logs',  # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        learning_rate=3e-4\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\")\n",
    "\n",
    "    # model: https://huggingface.co/transformers/v4.5.1/main_classes/model.html#transformers.PreTrainedModel.resize_token_embeddings\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],  # training dataset\n",
    "        eval_dataset=tokenized_dataset[\"test\"]  # evaluation dataset\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Training on\", device, torch.cuda.get_device_name)\n",
    "    model = trainer.model.to(device)\n",
    "    model.save_pretrained(\"../out/model/model_out_sequence.bin\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ae2f30e0ad00aa0d\n",
      "Reusing dataset text (/home/gnom/.cache/huggingface/datasets/text/default-ae2f30e0ad00aa0d/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n",
      "Loading cached split indices for dataset at /home/gnom/.cache/huggingface/datasets/text/default-ae2f30e0ad00aa0d/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-e6cd4c9b78814303.arrow and /home/gnom/.cache/huggingface/datasets/text/default-ae2f30e0ad00aa0d/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-c6fafa6a5c5ee5e0.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e9e92f429314890af70347aed6b52da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Erfahre to the vocabulary\n",
      "Adding Abheben to the vocabulary\n",
      "Adding Überschlagen to the vocabulary\n",
      "Adding Abtrocknen to the vocabulary\n",
      "Adding wisch to the vocabulary\n",
      "Adding gesell to the vocabulary\n",
      "Adding netz to the vocabulary\n",
      "Adding abschlepp to the vocabulary\n",
      "Adding reproduzie to the vocabulary\n",
      "Adding Abi to the vocabulary\n",
      "Adding superhippen to the vocabulary\n",
      "Adding implantie to the vocabulary\n",
      "Adding Meinst to the vocabulary\n",
      "Adding ##vorg to the vocabulary\n",
      "Adding andau to the vocabulary\n",
      "Adding Anmischen to the vocabulary\n",
      "Adding wint to the vocabulary\n",
      "Adding wipp to the vocabulary\n",
      "Adding dct to the vocabulary\n",
      "Adding anha to the vocabulary\n",
      "Adding Gönn to the vocabulary\n",
      "Adding Programmieren to the vocabulary\n",
      "Adding volume to the vocabulary\n",
      "Adding ##wimm to the vocabulary\n",
      "Adding Vermehrt to the vocabulary\n",
      "Adding komprimie to the vocabulary\n",
      "Adding wett to the vocabulary\n",
      "Adding ##zumach to the vocabulary\n",
      "Adding Eintauchen to the vocabulary\n",
      "Adding ##lanc to the vocabulary\n",
      "Adding anru to the vocabulary\n",
      "Adding erga to the vocabulary\n",
      "Adding auszu­bauen to the vocabulary\n",
      "Adding konzipier to the vocabulary\n",
      "Adding copyleft to the vocabulary\n",
      "Adding Dürfte to the vocabulary\n",
      "Adding Schnapp to the vocabulary\n",
      "Adding bell to the vocabulary\n",
      "Adding Fühl to the vocabulary\n",
      "Adding ##trennt to the vocabulary\n",
      "Adding Begleitet to the vocabulary\n",
      "Adding ou to the vocabulary\n",
      "Adding Weisse to the vocabulary\n",
      "Adding reih to the vocabulary\n",
      "Adding Zogen to the vocabulary\n",
      "Adding Vorstöße to the vocabulary\n",
      "Adding ##türmt to the vocabulary\n",
      "Adding schaf to the vocabulary\n",
      "Adding Anlassen to the vocabulary\n",
      "Adding verü to the vocabulary\n",
      "Adding Zusammenfallen to the vocabulary\n",
      "Adding flie to the vocabulary\n",
      "Adding inneha to the vocabulary\n",
      "Adding gepol to the vocabulary\n",
      "Adding MACHEN to the vocabulary\n",
      "Adding BEENDEN to the vocabulary\n",
      "Adding meck to the vocabulary\n",
      "Adding ##kopp to the vocabulary\n",
      "Adding wäsch to the vocabulary\n",
      "Adding Verlad to the vocabulary\n",
      "Adding digitalisi to the vocabulary\n",
      "Adding Treten to the vocabulary\n",
      "Adding endkundenkampagnen to the vocabulary\n",
      "Adding assoziiert to the vocabulary\n",
      "Adding ##espü to the vocabulary\n",
      "Adding 1St to the vocabulary\n",
      "Adding fortlassen to the vocabulary\n",
      "Adding ##gefei to the vocabulary\n",
      "Adding ##fleg to the vocabulary\n",
      "Adding ##erleg to the vocabulary\n",
      "Adding schädi to the vocabulary\n",
      "Adding ##zuzäh to the vocabulary\n",
      "Adding must to the vocabulary\n",
      "Adding 2015Ausgabe to the vocabulary\n",
      "Adding ##nflo to the vocabulary\n",
      "Adding ##geko to the vocabulary\n",
      "Adding Fahre to the vocabulary\n",
      "Adding Niederrasen to the vocabulary\n",
      "Adding koordinie to the vocabulary\n",
      "Adding ##büg to the vocabulary\n",
      "Adding Berechnen to the vocabulary\n",
      "Adding mitrech to the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff1bc26831914feba7de6437e3eaf79a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Schleich to the vocabulary\n",
      "Adding vielicht to the vocabulary\n",
      "Adding anla to the vocabulary\n",
      "Adding ##erlei to the vocabulary\n",
      "Adding myessentielleoele to the vocabulary\n",
      "Adding Lache to the vocabulary\n",
      "Adding Fichten to the vocabulary\n",
      "Adding Aufdecken to the vocabulary\n",
      "Adding geheu to the vocabulary\n",
      "Adding ##einstim to the vocabulary\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-german-cased/resolve/main/config.json from cache at /home/gnom/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-german-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-german-cased/resolve/main/pytorch_model.bin from cache at /home/gnom/.cache/huggingface/transformers/5236eea09283e87ba7c16d0571a12520ed4f076869f3d943fdbfaaa34b71e419.953a553bf3928a893b8cacf8d8c46ce6c565c095f062120aa0773821285cde25\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Configuration saved in ../out/model/model_out_sequence.bin/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../out/model/model_out_sequence.bin/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MAIN FUNCTION BODY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da626f050a6d4437bbd77c8774d056ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4448a85b07f627b2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/gnom/.cache/huggingface/datasets/text/default-4448a85b07f627b2/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0b53142757e4db190c99853f8e11a94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "661b0c153c5e44eb870d6a2bc83104b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0 tables [00:00, ? tables/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3b4e45333ec4a1f87da28a8bcdc93df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/gnom/.cache/huggingface/datasets/text/default-4448a85b07f627b2/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "files =  tu.files_from_path(\"../data/oscar/\", full_path=True)\n",
    "dataset = datasets.load_dataset(\"text\", data_files=files, split=\"train\")\n",
    "dataset = dataset.train_test_split(train_size=500000, test_size=75000, writer_batch_size=1000)\n",
    "metric = datasets.load_metric('glue', 'mrpc', keep_in_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def wm_tokenize(data):\n",
    "    return wmt.wordmap2tokenizer(data['text'], pos_tag=\"V\", vocab=wmt.vocab, pt=wmt.bert_pretokenizer, tk=wmt.bert_tokenizer, tg=wmt.hantatagger)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57b0359c9d3d4f878878ff66fed08c76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ee2304320574f1a9aae84f2ef5f23f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(wm_tokenize, batched=True, batch_size=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "result = wmt.wordmap2tokenizer(dataset[\"train\"][\"text\"][5:7], pos_tag=\"V\", vocab=wmt.vocab, pt=wmt.bert_pretokenizer, tk=wmt.bert_tokenizer,\n",
    "                                 tg=wmt.hantatagger)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['train', 'test'])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "500000"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"train\"][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
