{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch.cuda\n",
    "\n",
    "import wm_tokenizer\n",
    "import text_utilities as tu\n",
    "from HanTa import HanoverTagger as ht\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments , AutoModelForMaskedLM  # , AutoTokenizer, BertForMaskedLM\n",
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "import sklearn\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "vd = tu.VerbDict(\"../new_tokenizer/fun_vocab_raw.txt\", \"../new_tokenizer/lex_vocab_raw.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "wmt = wm_tokenizer.WordmapTokenizer(\n",
    "    bert_pretokenizer=pre_tokenizers.BertPreTokenizer(),\n",
    "    bert_tokenizer=BertTokenizer.from_pretrained(\"bert-base-german-cased\"),\n",
    "    hantatagger=ht.HanoverTagger('morphmodel_ger.pgz'),\n",
    "    vocab=vd.lmfm\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def wm_tokenize(data):\n",
    "    return wmt.wordmap2tokenizer(data['text'], pos_tag=\"V\", vocab=wmt.vocab, pt=wmt.bert_pretokenizer, tk=wmt.bert_tokenizer, tg=wmt.hantatagger)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"TODO: Fix UNK tokens bei wmt.SequenceTokenizer\"\"\"\n",
    "\n",
    "    files =  tu.files_from_path(\"../data/oscar/to_lines\", full_path=True)\n",
    "    dataset = datasets.load_dataset(\"text\", data_files=files[5:15], split=\"train\")\n",
    "    dataset = dataset.train_test_split(train_size=1000, test_size=150, writer_batch_size=100)\n",
    "    metric = datasets.load_metric('glue', 'mrpc', keep_in_memory=True)\n",
    "\n",
    "\n",
    "    tokenized_dataset = dataset.map(wm_tokenize, batched=True, batch_size=1000)\n",
    "\n",
    "    # recommendations: https://github.com/google-research/bert\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./out/model_out',  # output directory\n",
    "        num_train_epochs=4,  # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        logging_dir='./out/model_logs',  # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        learning_rate=3e-4\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\")\n",
    "\n",
    "    # model: https://huggingface.co/transformers/v4.5.1/main_classes/model.html#transformers.PreTrainedModel.resize_token_embeddings\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],  # training dataset\n",
    "        eval_dataset=tokenized_dataset[\"test\"]  # evaluation dataset\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model = trainer.model.to(device)\n",
    "    model.save_pretrained(\"../out/model/model_out_sequence.bin\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ae2f30e0ad00aa0d\n",
      "Reusing dataset text (/home/gnom/.cache/huggingface/datasets/text/default-ae2f30e0ad00aa0d/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n",
      "Loading cached split indices for dataset at /home/gnom/.cache/huggingface/datasets/text/default-ae2f30e0ad00aa0d/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-e6cd4c9b78814303.arrow and /home/gnom/.cache/huggingface/datasets/text/default-ae2f30e0ad00aa0d/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-c6fafa6a5c5ee5e0.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e9e92f429314890af70347aed6b52da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Erfahre to the vocabulary\n",
      "Adding Abheben to the vocabulary\n",
      "Adding Überschlagen to the vocabulary\n",
      "Adding Abtrocknen to the vocabulary\n",
      "Adding wisch to the vocabulary\n",
      "Adding gesell to the vocabulary\n",
      "Adding netz to the vocabulary\n",
      "Adding abschlepp to the vocabulary\n",
      "Adding reproduzie to the vocabulary\n",
      "Adding Abi to the vocabulary\n",
      "Adding superhippen to the vocabulary\n",
      "Adding implantie to the vocabulary\n",
      "Adding Meinst to the vocabulary\n",
      "Adding ##vorg to the vocabulary\n",
      "Adding andau to the vocabulary\n",
      "Adding Anmischen to the vocabulary\n",
      "Adding wint to the vocabulary\n",
      "Adding wipp to the vocabulary\n",
      "Adding dct to the vocabulary\n",
      "Adding anha to the vocabulary\n",
      "Adding Gönn to the vocabulary\n",
      "Adding Programmieren to the vocabulary\n",
      "Adding volume to the vocabulary\n",
      "Adding ##wimm to the vocabulary\n",
      "Adding Vermehrt to the vocabulary\n",
      "Adding komprimie to the vocabulary\n",
      "Adding wett to the vocabulary\n",
      "Adding ##zumach to the vocabulary\n",
      "Adding Eintauchen to the vocabulary\n",
      "Adding ##lanc to the vocabulary\n",
      "Adding anru to the vocabulary\n",
      "Adding erga to the vocabulary\n",
      "Adding auszu­bauen to the vocabulary\n",
      "Adding konzipier to the vocabulary\n",
      "Adding copyleft to the vocabulary\n",
      "Adding Dürfte to the vocabulary\n",
      "Adding Schnapp to the vocabulary\n",
      "Adding bell to the vocabulary\n",
      "Adding Fühl to the vocabulary\n",
      "Adding ##trennt to the vocabulary\n",
      "Adding Begleitet to the vocabulary\n",
      "Adding ou to the vocabulary\n",
      "Adding Weisse to the vocabulary\n",
      "Adding reih to the vocabulary\n",
      "Adding Zogen to the vocabulary\n",
      "Adding Vorstöße to the vocabulary\n",
      "Adding ##türmt to the vocabulary\n",
      "Adding schaf to the vocabulary\n",
      "Adding Anlassen to the vocabulary\n",
      "Adding verü to the vocabulary\n",
      "Adding Zusammenfallen to the vocabulary\n",
      "Adding flie to the vocabulary\n",
      "Adding inneha to the vocabulary\n",
      "Adding gepol to the vocabulary\n",
      "Adding MACHEN to the vocabulary\n",
      "Adding BEENDEN to the vocabulary\n",
      "Adding meck to the vocabulary\n",
      "Adding ##kopp to the vocabulary\n",
      "Adding wäsch to the vocabulary\n",
      "Adding Verlad to the vocabulary\n",
      "Adding digitalisi to the vocabulary\n",
      "Adding Treten to the vocabulary\n",
      "Adding endkundenkampagnen to the vocabulary\n",
      "Adding assoziiert to the vocabulary\n",
      "Adding ##espü to the vocabulary\n",
      "Adding 1St to the vocabulary\n",
      "Adding fortlassen to the vocabulary\n",
      "Adding ##gefei to the vocabulary\n",
      "Adding ##fleg to the vocabulary\n",
      "Adding ##erleg to the vocabulary\n",
      "Adding schädi to the vocabulary\n",
      "Adding ##zuzäh to the vocabulary\n",
      "Adding must to the vocabulary\n",
      "Adding 2015Ausgabe to the vocabulary\n",
      "Adding ##nflo to the vocabulary\n",
      "Adding ##geko to the vocabulary\n",
      "Adding Fahre to the vocabulary\n",
      "Adding Niederrasen to the vocabulary\n",
      "Adding koordinie to the vocabulary\n",
      "Adding ##büg to the vocabulary\n",
      "Adding Berechnen to the vocabulary\n",
      "Adding mitrech to the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff1bc26831914feba7de6437e3eaf79a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Schleich to the vocabulary\n",
      "Adding vielicht to the vocabulary\n",
      "Adding anla to the vocabulary\n",
      "Adding ##erlei to the vocabulary\n",
      "Adding myessentielleoele to the vocabulary\n",
      "Adding Lache to the vocabulary\n",
      "Adding Fichten to the vocabulary\n",
      "Adding Aufdecken to the vocabulary\n",
      "Adding geheu to the vocabulary\n",
      "Adding ##einstim to the vocabulary\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-german-cased/resolve/main/config.json from cache at /home/gnom/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-german-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-german-cased/resolve/main/pytorch_model.bin from cache at /home/gnom/.cache/huggingface/transformers/5236eea09283e87ba7c16d0571a12520ed4f076869f3d943fdbfaaa34b71e419.953a553bf3928a893b8cacf8d8c46ce6c565c095f062120aa0773821285cde25\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Configuration saved in ../out/model/model_out_sequence.bin/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../out/model/model_out_sequence.bin/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MAIN FUNCTION BODY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ae2f30e0ad00aa0d\n",
      "Reusing dataset text (/home/gnom/.cache/huggingface/datasets/text/default-ae2f30e0ad00aa0d/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    }
   ],
   "source": [
    "files =  tu.files_from_path(\"../data/oscar/to_lines\", full_path=True)\n",
    "dataset = datasets.load_dataset(\"text\", data_files=files[5:15], split=\"train\")\n",
    "dataset = dataset.train_test_split(train_size=1000, test_size=150, writer_batch_size=100)\n",
    "metric = datasets.load_metric('glue', 'mrpc', keep_in_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def wm_tokenize(data):\n",
    "    return wmt.wordmap2tokenizer(data['text'], pos_tag=\"V\", vocab=wmt.vocab, pt=wmt.bert_pretokenizer, tk=wmt.bert_tokenizer, tg=wmt.hantatagger)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57b0359c9d3d4f878878ff66fed08c76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ee2304320574f1a9aae84f2ef5f23f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(wm_tokenize, batched=True, batch_size=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "result = wmt.wordmap2tokenizer(dataset[\"train\"][\"text\"][5:7], pos_tag=\"V\", vocab=wmt.vocab, pt=wmt.bert_pretokenizer, tk=wmt.bert_tokenizer,\n",
    "                                 tg=wmt.hantatagger)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_407648/3744538699.py\", line 23, in <module>\n",
      "    bert_tokenizer=BertTokenizer.from_pretrained(\"bert-base-german-cased\"),\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 1752, in from_pretrained\n",
      "    user_agent=user_agent,\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/transformers/utils/hub.py\", line 292, in cached_path\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/transformers/utils/hub.py\", line 501, in get_from_cache\n",
      "    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/api.py\", line 100, in head\n",
      "    return request(\"head\", url, **kwargs)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/adapters.py\", line 499, in send\n",
      "    timeout=timeout,\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 710, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connection.py\", line 358, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connection.py\", line 175, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2098, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/gnom/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/inspect.py\", line 732, in getmodule\n",
      "    for modname, module in sys.modules.copy().items():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_407648/3744538699.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0mbert_pretokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpre_tokenizers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBertPreTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0mbert_tokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mBertTokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"bert-base-german-cased\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m     \u001B[0mhantatagger\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mht\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mHanoverTagger\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'morphmodel_ger.pgz'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1751\u001B[0m                         \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1752\u001B[0;31m                         \u001B[0muser_agent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muser_agent\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1753\u001B[0m                     )\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/transformers/utils/hub.py\u001B[0m in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    291\u001B[0m             \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 292\u001B[0;31m             \u001B[0mlocal_files_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlocal_files_only\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    293\u001B[0m         )\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/transformers/utils/hub.py\u001B[0m in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    500\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 501\u001B[0;31m             \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_redirects\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproxies\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0metag_timeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    502\u001B[0m             \u001B[0m_raise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/api.py\u001B[0m in \u001B[0;36mhead\u001B[0;34m(url, **kwargs)\u001B[0m\n\u001B[1;32m     99\u001B[0m     \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetdefault\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"allow_redirects\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 100\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"head\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/api.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0msessions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/sessions.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    586\u001B[0m         \u001B[0msend_kwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msettings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 587\u001B[0;31m         \u001B[0mresp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0msend_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/sessions.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;31m# Send the request\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 701\u001B[0;31m         \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madapter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    702\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/requests/adapters.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    498\u001B[0m                     \u001B[0mretries\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_retries\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 499\u001B[0;31m                     \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    500\u001B[0m                 )\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    709\u001B[0m                 \u001B[0mheaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 710\u001B[0;31m                 \u001B[0mchunked\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mchunked\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    711\u001B[0m             )\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    385\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 386\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_conn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    387\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mSocketTimeout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBaseSSLError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36m_validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1041\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"sock\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# AppEngine might not have  `.sock`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1042\u001B[0;31m             \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1043\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connection.py\u001B[0m in \u001B[0;36mconnect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    357\u001B[0m         \u001B[0;31m# Add certificate verification\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 358\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msock\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new_conn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    359\u001B[0m         \u001B[0mhostname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhost\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/connection.py\u001B[0m in \u001B[0;36m_new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    174\u001B[0m             conn = connection.create_connection(\n\u001B[0;32m--> 175\u001B[0;31m                 \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dns_host\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mport\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mextra_kw\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    176\u001B[0m             )\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/urllib3/util/connection.py\u001B[0m in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     84\u001B[0m                 \u001B[0msock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msource_address\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m             \u001B[0msock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msa\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0msock\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2097\u001B[0m                         \u001B[0;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2098\u001B[0;31m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2099\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2099\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2100\u001B[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[0;32m-> 2101\u001B[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001B[0m\u001B[1;32m   2102\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2103\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_showtraceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1366\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1367\u001B[0m         return FormattedTB.structured_traceback(\n\u001B[0;32m-> 1368\u001B[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[0m\u001B[1;32m   1369\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1370\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1266\u001B[0m             \u001B[0;31m# Verbose modes need a full traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1267\u001B[0m             return VerboseTB.structured_traceback(\n\u001B[0;32m-> 1268\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1269\u001B[0m             )\n\u001B[1;32m   1270\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'Minimal'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1123\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1124\u001B[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0;32m-> 1125\u001B[0;31m                                                                tb_offset)\n\u001B[0m\u001B[1;32m   1126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1127\u001B[0m         \u001B[0mcolors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mColors\u001B[0m  \u001B[0;31m# just a shorthand + quicker name lookup\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1081\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1082\u001B[0;31m         \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1084\u001B[0m         \u001B[0mframes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/builds/anaconda3/envs/tokenizer3.7/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[0;34m(etype, value, records)\u001B[0m\n\u001B[1;32m    380\u001B[0m     \u001B[0;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    384\u001B[0m     \u001B[0;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "__author__ = \"Ricardo Jung\"\n",
    "__email__ = \"s2458588@stud.uni-frankfurt.de\"\n",
    "\n",
    "# __copyright__ = \"\"\n",
    "# __credits__ = [\"\"]\n",
    "# __license__ = \"\"\n",
    "# __version__ = \"\"\n",
    "# __maintainer__ = \"\"\n",
    "# __status__ = \"\"\n",
    "\n",
    "import datasets\n",
    "import wm_tokenizer\n",
    "import text_utilities as tu\n",
    "from HanTa import HanoverTagger as ht\n",
    "from transformers import BertTokenizer, Trainer, TrainingArguments, BertForMaskedLM, AutoModelForMaskedLM\n",
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "vd = tu.VerbDict(\"../new_tokenizer/fun_vocab_raw.txt\", \"../new_tokenizer/lex_vocab_raw.txt\")\n",
    "\n",
    "wmt = wm_tokenizer.WordmapTokenizer(\n",
    "    bert_pretokenizer=pre_tokenizers.BertPreTokenizer(),\n",
    "    bert_tokenizer=BertTokenizer.from_pretrained(\"bert-base-german-cased\"),\n",
    "    hantatagger=ht.HanoverTagger('morphmodel_ger.pgz'),\n",
    "    vocab=vd.lmfm\n",
    ")\n",
    "\n",
    "\n",
    "def wm_tokenize(data):\n",
    "    return wmt.wordmap2tokenizer(data['text'], pos_tag=\"V\", vocab=wmt.vocab, pt=wmt.bert_pretokenizer,\n",
    "                                 tk=wmt.bert_tokenizer, tg=wmt.hantatagger)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    files = tu.files_from_path(\"../data/oscar/to_lines\", full_path=True)\n",
    "    dataset = datasets.load_dataset(\"text\", data_files=files[5:15], split=\"train\")\n",
    "    dataset = dataset.train_test_split(train_size=1000, test_size=150, writer_batch_size=100)\n",
    "    metric = datasets.load_metric('glue', 'mrpc', keep_in_memory=True)\n",
    "\n",
    "    tokenized_dataset = dataset.map(wm_tokenize, batched=True, batch_size=1000)\n",
    "\n",
    "    # recommendations: https://github.com/google-research/bert\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./out/model_out',  # output directory\n",
    "        num_train_epochs=4,  # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        logging_dir='./out/model_logs',  # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        learning_rate=3e-4\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\")\n",
    "\n",
    "    # model: https://huggingface.co/transformers/v4.5.1/main_classes/model.html#transformers.PreTrainedModel.resize_token_embeddings\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],  # training dataset\n",
    "        eval_dataset=tokenized_dataset[\"test\"]  # evaluation dataset\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = trainer.model.to(device)\n",
    "    model.save_pretrained(\"../out/model/test_model.model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ae2f30e0ad00aa0d\n",
      "Reusing dataset text (/home/gnom/.cache/huggingface/datasets/text/default-ae2f30e0ad00aa0d/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d26c5945a4564c7dbd6dd737bd8798c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding ##schlos to the vocabulary\n",
      "Adding füh to the vocabulary\n",
      "Adding wi to the vocabulary\n",
      "Adding ##rd to the vocabulary\n",
      "Adding ##schätzt to the vocabulary\n",
      "Adding tret to the vocabulary\n",
      "Adding biet to the vocabulary\n",
      "Adding Erfahre to the vocabulary\n",
      "Adding könn to the vocabulary\n",
      "Adding ##eht to the vocabulary\n",
      "Adding mach to the vocabulary\n",
      "Adding steh to the vocabulary\n",
      "Adding wol to the vocabulary\n",
      "Adding dach to the vocabulary\n",
      "Adding ##rb to the vocabulary\n",
      "Adding ##steh to the vocabulary\n",
      "Adding ##bind to the vocabulary\n",
      "Adding Abheben to the vocabulary\n",
      "Adding Überschlagen to the vocabulary\n",
      "Adding vermei to the vocabulary\n",
      "Adding Abtrocknen to the vocabulary\n",
      "Adding wisch to the vocabulary\n",
      "Adding möch to the vocabulary\n",
      "Adding gesell to the vocabulary\n",
      "Adding ##zuta to the vocabulary\n",
      "Adding bege to the vocabulary\n",
      "Adding ##rte to the vocabulary\n",
      "Adding netz to the vocabulary\n",
      "Adding offenba to the vocabulary\n",
      "Adding Seh to the vocabulary\n",
      "Adding seh to the vocabulary\n",
      "Adding abschlepp to the vocabulary\n",
      "Adding gib to the vocabulary\n",
      "Adding ##eitert to the vocabulary\n",
      "Adding ##biet to the vocabulary\n",
      "Adding stel to the vocabulary\n",
      "Adding reproduzie to the vocabulary\n",
      "Adding beantwo to the vocabulary\n",
      "Adding errei to the vocabulary\n",
      "Adding bestim to the vocabulary\n",
      "Adding Abi to the vocabulary\n",
      "Adding fei to the vocabulary\n",
      "Adding superhippen to the vocabulary\n",
      "Adding sag to the vocabulary\n",
      "Adding Erstellen to the vocabulary\n",
      "Adding speich to the vocabulary\n",
      "Adding implantie to the vocabulary\n",
      "Adding registrie to the vocabulary\n",
      "Adding Meinst to the vocabulary\n",
      "Adding läs to the vocabulary\n",
      "Adding wechs to the vocabulary\n",
      "Adding ##zugeh to the vocabulary\n",
      "Adding bedien to the vocabulary\n",
      "Adding ##vorg to the vocabulary\n",
      "Adding erklä to the vocabulary\n",
      "Adding gestalte to the vocabulary\n",
      "Adding andau to the vocabulary\n",
      "Adding ##eug to the vocabulary\n",
      "Adding ##egt to the vocabulary\n",
      "Adding ers to the vocabulary\n",
      "Adding ##chei to the vocabulary\n",
      "Adding Anmischen to the vocabulary\n",
      "Adding dring to the vocabulary\n",
      "Adding prüf to the vocabulary\n",
      "Adding würd to the vocabulary\n",
      "Adding au to the vocabulary\n",
      "Adding wint to the vocabulary\n",
      "Adding ##erh to the vocabulary\n",
      "Adding begin to the vocabulary\n",
      "Adding wipp to the vocabulary\n",
      "Adding hüpf to the vocabulary\n",
      "Adding sollt to the vocabulary\n",
      "Adding dct to the vocabulary\n",
      "Adding schei to the vocabulary\n",
      "Adding vermute to the vocabulary\n",
      "Adding Anfahren to the vocabulary\n",
      "Adding folg to the vocabulary\n",
      "Adding ##grü to the vocabulary\n",
      "Adding ##ndet to the vocabulary\n",
      "Adding ##fing to the vocabulary\n",
      "Adding anha to the vocabulary\n",
      "Adding find to the vocabulary\n",
      "Adding buch to the vocabulary\n",
      "Adding trei to the vocabulary\n",
      "Adding verge to the vocabulary\n",
      "Adding verwe to the vocabulary\n",
      "Adding spi to the vocabulary\n",
      "Adding ##bitt to the vocabulary\n",
      "Adding zieh to the vocabulary\n",
      "Adding brau to the vocabulary\n",
      "Adding ma to the vocabulary\n",
      "Adding ##timm to the vocabulary\n",
      "Adding Gönn to the vocabulary\n",
      "Adding bewäh to the vocabulary\n",
      "Adding ##ausgin to the vocabulary\n",
      "Adding geseh to the vocabulary\n",
      "Adding lob to the vocabulary\n",
      "Adding ##nannt to the vocabulary\n",
      "Adding fas to the vocabulary\n",
      "Adding ##tig to the vocabulary\n",
      "Adding Programmieren to the vocabulary\n",
      "Adding nu to the vocabulary\n",
      "Adding gla to the vocabulary\n",
      "Adding festgestel to the vocabulary\n",
      "Adding ##kannt to the vocabulary\n",
      "Adding beei to the vocabulary\n",
      "Adding volume to the vocabulary\n",
      "Adding konnt to the vocabulary\n",
      "Adding ka to the vocabulary\n",
      "Adding hatt to the vocabulary\n",
      "Adding ##meld to the vocabulary\n",
      "Adding ermitt to the vocabulary\n",
      "Adding übe to the vocabulary\n",
      "Adding ##rz to the vocabulary\n",
      "Adding änd to the vocabulary\n",
      "Adding fortfah to the vocabulary\n",
      "Adding anz to the vocabulary\n",
      "Adding gr to the vocabulary\n",
      "Adding ##inse to the vocabulary\n",
      "Adding fah to the vocabulary\n",
      "Adding ##wimm to the vocabulary\n",
      "Adding ##öffn to the vocabulary\n",
      "Adding ##sag to the vocabulary\n",
      "Adding ##tatt to the vocabulary\n",
      "Adding bea to the vocabulary\n",
      "Adding ##ntra to the vocabulary\n",
      "Adding ##nha to the vocabulary\n",
      "Adding gezog to the vocabulary\n",
      "Adding Vermehrt to the vocabulary\n",
      "Adding interessie to the vocabulary\n",
      "Adding ##rst to the vocabulary\n",
      "Adding komprimie to the vocabulary\n",
      "Adding ##klick to the vocabulary\n",
      "Adding ##schos to the vocabulary\n",
      "Adding verbu to the vocabulary\n",
      "Adding ##nden to the vocabulary\n",
      "Adding verges to the vocabulary\n",
      "Adding bezah to the vocabulary\n",
      "Adding hi to the vocabulary\n",
      "Adding ##lf to the vocabulary\n",
      "Adding wett to the vocabulary\n",
      "Adding ##zumach to the vocabulary\n",
      "Adding pas to the vocabulary\n",
      "Adding ang to the vocabulary\n",
      "Adding ##eze to the vocabulary\n",
      "Adding darstel to the vocabulary\n",
      "Adding ##tret to the vocabulary\n",
      "Adding ##barte to the vocabulary\n",
      "Adding gelt to the vocabulary\n",
      "Adding ref to the vocabulary\n",
      "Adding bedürf to the vocabulary\n",
      "Adding Eintauchen to the vocabulary\n",
      "Adding ##mis to the vocabulary\n",
      "Adding sorg to the vocabulary\n",
      "Adding test to the vocabulary\n",
      "Adding ##nst to the vocabulary\n",
      "Adding ##rui to the vocabulary\n",
      "Adding ##cho to the vocabulary\n",
      "Adding verka to the vocabulary\n",
      "Adding ##brin to the vocabulary\n",
      "Adding ##tarb to the vocabulary\n",
      "Adding fäll to the vocabulary\n",
      "Adding ba to the vocabulary\n",
      "Adding ##lanc to the vocabulary\n",
      "Adding wis to the vocabulary\n",
      "Adding ##wäss to the vocabulary\n",
      "Adding hochge to the vocabulary\n",
      "Adding bewah to the vocabulary\n",
      "Adding kümm to the vocabulary\n",
      "Adding fließ to the vocabulary\n",
      "Adding ##zulas to the vocabulary\n",
      "Adding stöb to the vocabulary\n",
      "Adding anru to the vocabulary\n",
      "Adding antwort to the vocabulary\n",
      "Adding müss to the vocabulary\n",
      "Adding herge to the vocabulary\n",
      "Adding schau to the vocabulary\n",
      "Adding ford to the vocabulary\n",
      "Adding stütz to the vocabulary\n",
      "Adding gehö to the vocabulary\n",
      "Adding ##rf to the vocabulary\n",
      "Adding gestorb to the vocabulary\n",
      "Adding erga to the vocabulary\n",
      "Adding gefal to the vocabulary\n",
      "Adding ##ständigt to the vocabulary\n",
      "Adding feh to the vocabulary\n",
      "Adding abb to the vocabulary\n",
      "Adding möcht to the vocabulary\n",
      "Adding ##nötig to the vocabulary\n",
      "Adding ##tie to the vocabulary\n",
      "Adding ##günsti to the vocabulary\n",
      "Adding auszu­bauen to the vocabulary\n",
      "Adding konzipier to the vocabulary\n",
      "Adding fortge to the vocabulary\n",
      "Adding copyleft to the vocabulary\n",
      "Adding bitt to the vocabulary\n",
      "Adding sprech to the vocabulary\n",
      "Adding mi to the vocabulary\n",
      "Adding ##nimie to the vocabulary\n",
      "Adding ##sich to the vocabulary\n",
      "Adding Dürfte to the vocabulary\n",
      "Adding ##fit to the vocabulary\n",
      "Adding könnt to the vocabulary\n",
      "Adding widm to the vocabulary\n",
      "Adding Schnapp to the vocabulary\n",
      "Adding bri to the vocabulary\n",
      "Adding ##ng to the vocabulary\n",
      "Adding ##breitet to the vocabulary\n",
      "Adding ze to the vocabulary\n",
      "Adding prob to the vocabulary\n",
      "Adding erzäh to the vocabulary\n",
      "Adding ##gezog to the vocabulary\n",
      "Adding Anmelden to the vocabulary\n",
      "Adding Registrieren to the vocabulary\n",
      "Adding Musst to the vocabulary\n",
      "Adding spa to the vocabulary\n",
      "Adding tauch to the vocabulary\n",
      "Adding gewah to the vocabulary\n",
      "Adding abse to the vocabulary\n",
      "Adding aktivieren to the vocabulary\n",
      "Adding fess to the vocabulary\n",
      "Adding loh to the vocabulary\n",
      "Adding nehm to the vocabulary\n",
      "Adding stink to the vocabulary\n",
      "Adding bell to the vocabulary\n",
      "Adding schrei to the vocabulary\n",
      "Adding kündig to the vocabulary\n",
      "Adding geta to the vocabulary\n",
      "Adding erfas to the vocabulary\n",
      "Adding ##fand to the vocabulary\n",
      "Adding Fühl to the vocabulary\n",
      "Adding ##einträchti to the vocabulary\n",
      "Adding konn to the vocabulary\n",
      "Adding ##gla to the vocabulary\n",
      "Adding fan to the vocabulary\n",
      "Adding dürf to the vocabulary\n",
      "Adding back to the vocabulary\n",
      "Adding ##nk to the vocabulary\n",
      "Adding ##ulie to the vocabulary\n",
      "Adding ##zuruf to the vocabulary\n",
      "Adding studi to the vocabulary\n",
      "Adding rede to the vocabulary\n",
      "Adding gesproch to the vocabulary\n",
      "Adding Informieren to the vocabulary\n",
      "Adding kan to the vocabulary\n",
      "Adding gewin to the vocabulary\n",
      "Adding spra to the vocabulary\n",
      "Adding bleib to the vocabulary\n",
      "Adding ableh to the vocabulary\n",
      "Adding abg to the vocabulary\n",
      "Adding wart to the vocabulary\n",
      "Adding nei to the vocabulary\n",
      "Adding reduzier to the vocabulary\n",
      "Adding umg to the vocabulary\n",
      "Adding ##ückge to the vocabulary\n",
      "Adding verlas to the vocabulary\n",
      "Adding fauch to the vocabulary\n",
      "Adding zubereit to the vocabulary\n",
      "Adding gefr to the vocabulary\n",
      "Adding ##stech to the vocabulary\n",
      "Adding ##günstig to the vocabulary\n",
      "Adding mes to the vocabulary\n",
      "Adding ##sorg to the vocabulary\n",
      "Adding ##chö to the vocabulary\n",
      "Adding äuß to the vocabulary\n",
      "Adding ##trennt to the vocabulary\n",
      "Adding ##rwie to the vocabulary\n",
      "Adding einle to the vocabulary\n",
      "Adding weig to the vocabulary\n",
      "Adding tr to the vocabulary\n",
      "Adding ##äg to the vocabulary\n",
      "Adding vo to the vocabulary\n",
      "Adding feststell to the vocabulary\n",
      "Adding ##küh to the vocabulary\n",
      "Adding ##eiz to the vocabulary\n",
      "Adding gene to the vocabulary\n",
      "Adding wär to the vocabulary\n",
      "Adding mög to the vocabulary\n",
      "Adding set to the vocabulary\n",
      "Adding stei to the vocabulary\n",
      "Adding ster to the vocabulary\n",
      "Adding ##süß to the vocabulary\n",
      "Adding ##erl to the vocabulary\n",
      "Adding brin to the vocabulary\n",
      "Adding Begleitet to the vocabulary\n",
      "Adding ##teu to the vocabulary\n",
      "Adding ou to the vocabulary\n",
      "Adding ni to the vocabulary\n",
      "Adding Weisse to the vocabulary\n",
      "Adding reih to the vocabulary\n",
      "Adding ##rk to the vocabulary\n",
      "Adding Zogen to the vocabulary\n",
      "Adding erlau to the vocabulary\n",
      "Adding Absenden to the vocabulary\n",
      "Adding ##peich to the vocabulary\n",
      "Adding fokussier to the vocabulary\n",
      "Adding ##eif to the vocabulary\n",
      "Adding Vorstöße to the vocabulary\n",
      "Adding ##türmt to the vocabulary\n",
      "Adding erhöh to the vocabulary\n",
      "Adding nut to the vocabulary\n",
      "Adding sprich to the vocabulary\n",
      "Adding bli to the vocabulary\n",
      "Adding dü to the vocabulary\n",
      "Adding ##chwu to the vocabulary\n",
      "Adding schaf to the vocabulary\n",
      "Adding nimm to the vocabulary\n",
      "Adding such to the vocabulary\n",
      "Adding Anlassen to the vocabulary\n",
      "Adding freu to the vocabulary\n",
      "Adding erlei to the vocabulary\n",
      "Adding ##ückg to the vocabulary\n",
      "Adding sieh to the vocabulary\n",
      "Adding wünsch to the vocabulary\n",
      "Adding ende to the vocabulary\n",
      "Adding umfas to the vocabulary\n",
      "Adding ##widm to the vocabulary\n",
      "Adding dau to the vocabulary\n",
      "Adding verü to the vocabulary\n",
      "Adding ##bewah to the vocabulary\n",
      "Adding handel to the vocabulary\n",
      "Adding ##rden to the vocabulary\n",
      "Adding rechn to the vocabulary\n",
      "Adding flüst to the vocabulary\n",
      "Adding treff to the vocabulary\n",
      "Adding Zusammenfallen to the vocabulary\n",
      "Adding lach to the vocabulary\n",
      "Adding ##rn to the vocabulary\n",
      "Adding flie to the vocabulary\n",
      "Adding gewund to the vocabulary\n",
      "Adding abschließ to the vocabulary\n",
      "Adding inneha to the vocabulary\n",
      "Adding pfle to the vocabulary\n",
      "Adding ##gelei to the vocabulary\n",
      "Adding hätt to the vocabulary\n",
      "Adding gepol to the vocabulary\n",
      "Adding erho to the vocabulary\n",
      "Adding ##nimm to the vocabulary\n",
      "Adding lös to the vocabulary\n",
      "Adding sitz to the vocabulary\n",
      "Adding kritisier to the vocabulary\n",
      "Adding beschrei to the vocabulary\n",
      "Adding MACHEN to the vocabulary\n",
      "Adding BEENDEN to the vocabulary\n",
      "Adding erschein to the vocabulary\n",
      "Adding stemm to the vocabulary\n",
      "Adding ##förd to the vocabulary\n",
      "Adding meck to the vocabulary\n",
      "Adding ##erle to the vocabulary\n",
      "Adding wan to the vocabulary\n",
      "Adding häng to the vocabulary\n",
      "Adding ##kopp to the vocabulary\n",
      "Adding ausrei to the vocabulary\n",
      "Adding wäsch to the vocabulary\n",
      "Adding pflegt to the vocabulary\n",
      "Adding ##prüft to the vocabulary\n",
      "Adding tu to the vocabulary\n",
      "Adding Bedienen to the vocabulary\n",
      "Adding ##eho to the vocabulary\n",
      "Adding ##nomm to the vocabulary\n",
      "Adding Verlad to the vocabulary\n",
      "Adding digitalisi to the vocabulary\n",
      "Adding Treten to the vocabulary\n",
      "Adding ##wus to the vocabulary\n",
      "Adding ##wiss to the vocabulary\n",
      "Adding überge to the vocabulary\n",
      "Adding ##waffn to the vocabulary\n",
      "Adding endkundenkampagnen to the vocabulary\n",
      "Adding umgeb to the vocabulary\n",
      "Adding ertei to the vocabulary\n",
      "Adding assoziiert to the vocabulary\n",
      "Adding ##gib to the vocabulary\n",
      "Adding ##spri to the vocabulary\n",
      "Adding räu to the vocabulary\n",
      "Adding nachg to the vocabulary\n",
      "Adding ##espü to the vocabulary\n",
      "Adding ##zufi to the vocabulary\n",
      "Adding beschnei to the vocabulary\n",
      "Adding taus to the vocabulary\n",
      "Adding spü to the vocabulary\n",
      "Adding gefro to the vocabulary\n",
      "Adding vere to the vocabulary\n",
      "Adding ##infa to the vocabulary\n",
      "Adding unterrüh to the vocabulary\n",
      "Adding bedank to the vocabulary\n",
      "Adding Füge to the vocabulary\n",
      "Adding 1St to the vocabulary\n",
      "Adding ##ege to the vocabulary\n",
      "Adding ##wach to the vocabulary\n",
      "Adding ##zäh to the vocabulary\n",
      "Adding fortlassen to the vocabulary\n",
      "Adding Kipp to the vocabulary\n",
      "Adding ##inba to the vocabulary\n",
      "Adding ##gefei to the vocabulary\n",
      "Adding zwing to the vocabulary\n",
      "Adding gebrau to the vocabulary\n",
      "Adding zerstö to the vocabulary\n",
      "Adding ##lüpf to the vocabulary\n",
      "Adding ruf to the vocabulary\n",
      "Adding ##mög to the vocabulary\n",
      "Adding ##fleg to the vocabulary\n",
      "Adding behand to the vocabulary\n",
      "Adding ##erleg to the vocabulary\n",
      "Adding schädi to the vocabulary\n",
      "Adding ##zuzäh to the vocabulary\n",
      "Adding spren to the vocabulary\n",
      "Adding must to the vocabulary\n",
      "Adding inspi to the vocabulary\n",
      "Adding erfül to the vocabulary\n",
      "Adding trenn to the vocabulary\n",
      "Adding arbeite to the vocabulary\n",
      "Adding ##tauch to the vocabulary\n",
      "Adding einl to the vocabulary\n",
      "Adding ##fun to the vocabulary\n",
      "Adding gerech to the vocabulary\n",
      "Adding ##fül to the vocabulary\n",
      "Adding ##dru to the vocabulary\n",
      "Adding ##miet to the vocabulary\n",
      "Adding glied to the vocabulary\n",
      "Adding 2015Ausgabe to the vocabulary\n",
      "Adding wüns to the vocabulary\n",
      "Adding ##zeichn to the vocabulary\n",
      "Adding verlo to the vocabulary\n",
      "Adding ##nflo to the vocabulary\n",
      "Adding heb to the vocabulary\n",
      "Adding erzi to the vocabulary\n",
      "Adding ##teh to the vocabulary\n",
      "Adding ##schrie to the vocabulary\n",
      "Adding erwa to the vocabulary\n",
      "Adding ernäh to the vocabulary\n",
      "Adding tei to the vocabulary\n",
      "Adding Klick to the vocabulary\n",
      "Adding neh to the vocabulary\n",
      "Adding funktionie to the vocabulary\n",
      "Adding wähl to the vocabulary\n",
      "Adding künd to the vocabulary\n",
      "Adding ##geko to the vocabulary\n",
      "Adding fürcht to the vocabulary\n",
      "Adding ##bekom to the vocabulary\n",
      "Adding Fahre to the vocabulary\n",
      "Adding Finde to the vocabulary\n",
      "Adding fun to the vocabulary\n",
      "Adding ##mog to the vocabulary\n",
      "Adding eta to the vocabulary\n",
      "Adding Holen to the vocabulary\n",
      "Adding ##eden to the vocabulary\n",
      "Adding Niederrasen to the vocabulary\n",
      "Adding Nimm to the vocabulary\n",
      "Adding limit to the vocabulary\n",
      "Adding gegang to the vocabulary\n",
      "Adding koordinie to the vocabulary\n",
      "Adding ##büg to the vocabulary\n",
      "Adding Berechnen to the vocabulary\n",
      "Adding stam to the vocabulary\n",
      "Adding ##zustel to the vocabulary\n",
      "Adding begrüß to the vocabulary\n",
      "Adding zuläs to the vocabulary\n",
      "Adding gebot to the vocabulary\n",
      "Adding bedruck to the vocabulary\n",
      "Adding ##zuri to the vocabulary\n",
      "Adding fäh to the vocabulary\n",
      "Adding ##füg to the vocabulary\n",
      "Adding mitrech to the vocabulary\n",
      "Adding geschaff to the vocabulary\n",
      "Adding gewinn to the vocabulary\n",
      "Adding wollt to the vocabulary\n",
      "Adding ##taill to the vocabulary\n",
      "Adding ##bess to the vocabulary\n",
      "Adding id to the vocabulary\n",
      "Adding ##ifi to the vocabulary\n",
      "Adding herrs to the vocabulary\n",
      "Adding mis to the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1e1d00450dc4be0a35e264a6423460e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Schleich to the vocabulary\n",
      "Adding rutsch to the vocabulary\n",
      "Adding fors to the vocabulary\n",
      "Adding vielicht to the vocabulary\n",
      "Adding Nachnahme to the vocabulary\n",
      "Adding ##sandt to the vocabulary\n",
      "Adding ##spi to the vocabulary\n",
      "Adding geko to the vocabulary\n",
      "Adding werf to the vocabulary\n",
      "Adding gewonn to the vocabulary\n",
      "Adding ##ekl to the vocabulary\n",
      "Adding Tasten to the vocabulary\n",
      "Adding red to the vocabulary\n",
      "Adding entla to the vocabulary\n",
      "Adding getrunk to the vocabulary\n",
      "Adding leh to the vocabulary\n",
      "Adding erhi to the vocabulary\n",
      "Adding benut to the vocabulary\n",
      "Adding anla to the vocabulary\n",
      "Adding ##edi to the vocabulary\n",
      "Adding ##gleit to the vocabulary\n",
      "Adding ##erlei to the vocabulary\n",
      "Adding Lasst to the vocabulary\n",
      "Adding erfuh to the vocabulary\n",
      "Adding myessentielleoele to the vocabulary\n",
      "Adding Lache to the vocabulary\n",
      "Adding Fichten to the vocabulary\n",
      "Adding Aufdecken to the vocabulary\n",
      "Adding ##tsch to the vocabulary\n",
      "Adding bilde to the vocabulary\n",
      "Adding herstel to the vocabulary\n",
      "Adding ##zuhalt to the vocabulary\n",
      "Adding strah to the vocabulary\n",
      "Adding fül to the vocabulary\n",
      "Adding geheu to the vocabulary\n",
      "Adding ##leu to the vocabulary\n",
      "Adding ##nz to the vocabulary\n",
      "Adding protokolli to the vocabulary\n",
      "Adding Sag to the vocabulary\n",
      "Adding ##einstim to the vocabulary\n",
      "Adding Kombinieren to the vocabulary\n",
      "Adding Stiefel to the vocabulary\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-german-cased/resolve/main/config.json from cache at /home/gnom/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-german-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-german-cased/resolve/main/pytorch_model.bin from cache at /home/gnom/.cache/huggingface/transformers/5236eea09283e87ba7c16d0571a12520ed4f076869f3d943fdbfaaa34b71e419.953a553bf3928a893b8cacf8d8c46ce6c565c095f062120aa0773821285cde25\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Configuration saved in ../out/model/test_model.model/config.json\n",
      "Model weights saved in ../out/model/test_model.model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
