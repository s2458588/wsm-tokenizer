{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import text_utilities as tu\n",
    "import regex as rex\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys([3, 1, 2, 4, 5, 6, 7, 8])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pc = tu.PosCorpus('../data/experiment/verbs')\n",
    "pc.counted_corpus.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class WordMapper:\n",
    "    \"\"\"Generates a Wordmap for a target token, comparing it to its POS-members in a dict sorted by syllables\"\"\"\n",
    "    def __init__(self, target: str, tokenset: dict, clean=True, pattern='([^^1][0*1*]+[^$1])'):\n",
    "        self.target = target\n",
    "        self.tokenset = tokenset\n",
    "        self.syllables = tu.count_syllables(target)\n",
    "        self.maps = self.stack_maps(target, tokenset, self.syllables)\n",
    "        self.pattern = pattern\n",
    "        self.clean_maps = None\n",
    "        if clean:\n",
    "            self.filter_map_noise(self.maps, self.pattern)\n",
    "            self.wordmap = self.sum_map_stack(self.clean_maps)\n",
    "        else:\n",
    "            self.wordmap = self.sum_map_stack(self.maps)\n",
    "\n",
    "\n",
    "    def stack_maps(self, target, tokenset, syllables):\n",
    "        l = len(target)\n",
    "        #cc1, cc2, cc3 = 0,0,0\n",
    "        maps = []\n",
    "        for k in tokenset:\n",
    "            for v in tokenset[k]:\n",
    "                pair = (v, self.target)\n",
    "                case = tu.match_ends(v, target)\n",
    "                shorter = min(pair, key=len)\n",
    "                longer = max(pair, key=len)\n",
    "                diff = len(longer) - len(shorter)\n",
    "\n",
    "                if case.get(\"any\"):\n",
    "                    if diff:\n",
    "                        if case.get(\"first\") and syllables!=1:\n",
    "                            #cc1+=1\n",
    "                            wm = tu.wordmap(longer=longer, shorter=shorter)\n",
    "                            while len(wm) < l:\n",
    "                                wm.append(0) # padding\n",
    "                            maps.append(wm)\n",
    "\n",
    "                        if case.get(\"last\"):\n",
    "                            wm = []\n",
    "                            #cc2+=1\n",
    "                            wm = tu.wordmap(longer=longer, shorter=shorter, start=diff)\n",
    "                            while len(wm) < l:\n",
    "                                wm.insert(0, 0) # padding\n",
    "                            maps.append(wm)\n",
    "\n",
    "                    else:\n",
    "                        #cc3+=1\n",
    "                        wm = tu.wordmap(longer=pair[0], shorter=pair[1])\n",
    "                        maps.append(wm)\n",
    "        #print(\"Cases:\", cc1, cc2, cc3)\n",
    "        return maps\n",
    "\n",
    "\n",
    "    def filter_map_noise(self, maps, pattern):\n",
    "        \"\"\"Convert maps to strings and delete any consecutive '1' not at the start or end of the map\"\"\"\n",
    "        str_maps = [\"\".join([str(c) for c in m]) for m in maps]  # cast to str\n",
    "        recount_map = [rex.sub(pattern=pattern, repl=lambda m: len(m.group(1))*\"0\",string=sm) for sm in str_maps]  # regex sub\n",
    "        regexed_listed = [list(i) for i in recount_map]  # into list form\n",
    "        regexed_inted = [[int(c) for c in m] for m in regexed_listed]  # cast back to int\n",
    "        self.clean_maps = regexed_inted\n",
    "\n",
    "    def sum_map_stack(self, maps):\n",
    "        return [sum(x) for x in zip(*maps)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "[6407, 5622, 5578, 256, 57, 319, 623, 24793]"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordmapper = WordMapper(\"verdutzt\", pc.counted_corpus, clean=True)\n",
    "wordmapper.wordmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "class MapToken:\n",
    "    \"\"\"Holds information about a single token. metrics must be text_utilities.PosCorpus metrics dict\"\"\"\n",
    "    def __init__(self, token: str, wordmap: list, metrics):\n",
    "        self.wordmap = wordmap\n",
    "        self.freqmap = {c:metrics[c] for c in metrics if c in token}\n",
    "        self.str = token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "mt = MapToken(wordmapper.target, wordmapper.wordmap, pc.metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "{'e': (129905, 0.16219310599704342),\n 't': (77211, 0.09640192376842863),\n 'r': (68048, 0.08496144472412002),\n 'u': (29988, 0.03744156778137361),\n 'd': (14059, 0.01755338806983899),\n 'z': (12020, 0.015007591194214711),\n 'v': (8892, 0.011102121539014742)}"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.freqmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##en\n"
     ]
    }
   ],
   "source": [
    "def map_subword(target: str, map:str) -> str:\n",
    "    \"\"\"Returns a subword from a target string and a map. Yet to implement maps with 1 on both ends.\"\"\"\n",
    "    if map.startswith(\"1\"):\n",
    "        return target[:map.count(\"1\")]+\"##\"\n",
    "    elif map.endswith(\"1\"):\n",
    "        return \"##\"+target[-map.count(\"1\"):]\n",
    "print(map_subword(\"verstehen\", \"000000011\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
